{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Principal Component Analysis (PPCA)\n",
    "\n",
    "~~~\n",
    "Extracción de Características en Imágenes.\n",
    "Master en Ciencia de Datos y Arquitectura de los Computadores.\n",
    "Universidad de Granada.\n",
    "\n",
    "\n",
    "Fernando Pérez Bueno - fpb@ugr.es\n",
    "Rafael Molina Soriano - rms@decsai.ugr.es\n",
    "~~~\n",
    "\n",
    "En este guión, vamos a estudiar la versión probabilistica del análisis de componentes principales. Esto nos va a permitir, por un lado, mirar al modelo PCA que hemos estudiado desde otro punto de vista (modelo generativo) y, por otro,  introducir un modelo  que nos va a permitir avanzar hacia los Variational AutoEncoders (VAEs) y Generative Adversarial Networks (GANs).\n",
    "\n",
    "Vamos a trabajar con el mismo dataset de caras de la practica de PCA. Veremos qué ocurre cuando tenemos que aprender el modelo con datos ruidosos.\n",
    "\n",
    "Como hemos visto en teoría, PPCA también utiliza los autovalores y autovectores, por lo que tendrás que reutilizar parte del código de la práctica anterior.\n",
    "\n",
    "Hemos visto en teoría que para cada observación tenemos una variable latente 'implícita' (que no observamos) z con P componentes donde P es el número de componentes principales escogido.\n",
    "\n",
    "Partimos de la modelización de nuestros datos observados x como:\n",
    "\n",
    "1. introducimos una distribución a priori gaussiana p(z) sobre la variable latente, de ella extraemos una muestra z, \n",
    "2. con ella generamos nuestras observaciones usando un modelo de observación, en nuestro caso lineal y con ruido Gaussiano,  p(x|z).\n",
    "\n",
    "Para ser precisos (suponemos que la media del modelo de observación es cero): \n",
    "\n",
    "\\begin{align*}\n",
    "p(z) &= \\mathcal{N}(0,I)\\\\\n",
    "p(x|z) &= \\mathcal{N}(x|Wz,\\sigma^2I)\\\n",
    "\\end{align*}\n",
    "\n",
    "Para que fijes los conceptos: en la clase de teoría sobre PPCA hemos supuesto que tenemos N ejemplos, cada uno con D componentes. Usaremos M para notar el número de componentes principales. Observa que, siguiendo la teoría, $W$ tiene tamaño DxM y $\\sigma^2$ controla la varianza de la distribución condicionada (cuanto ruido tenemos en las observaciones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias que vamos a utilizar en el desarrollo de la práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los datos. Recuerda modificar el camino para apuntar a tu fichero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/work/ECI_master/Data/'\n",
    "\n",
    "dict_data = loadmat(path+'ERRDfaces_2021.mat')\n",
    "data= dict_data['X']\n",
    "\n",
    "print(\"Tenemos {} muestras, cada una con {} componentes.\\n\\\n",
    "Cada muestra es un vector unidimensional.\".format(data.shape[0],data.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Añadiendo ruido a los datos\n",
    "\n",
    "A continuación vamos a añadir ruido gaussiano a cada una de las componentes de las instancias de nuestro problema. A todas las caras, tanto de entrenamiento como de test. Luego haremos la partición.\n",
    "\n",
    "Comenzamos obteniendo información sobre nuestros datos, su tipo y valores máximo y mínimo.\n",
    "\n",
    "Inicializamos el generador de números aleatorios para que todos generemos los mismos datos de ruido. \n",
    "\n",
    "A continuación fijamos la varianza del ruido a 10. Observa que nuestros datos están aproximadamente en el rango $[-127.5,127.5]$ . Piensa el efecto de añadirle ruido de varianza 10. Piensa el rango en el que el ruido está en el 95% de los casos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información sobre los datos\n",
    "\n",
    "mensaje=\"El tipo de dato es {}. Su máximo y mínimo son \\\n",
    "{:.2f} y {:.2f}, respectivamente\"\n",
    "print(mensaje.format(data.dtype,data.max(),data.min()))#Añadir ruido a los datos\n",
    "\n",
    "# Inicializamos la semilla del ruido y le añadimos ruido a los datos\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "var_ruido=10\n",
    "ruido=np.random.normal(0,var_ruido,data.shape)\n",
    "\n",
    "data_noisy=data+ruido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hiciste en la práctica anterior, redimensionamos los datos, es decir, pasamos de un vector con 1024 componentes a una matriz 32x32, y mostramos las 5 primeras caras del dataset sin ruido en la primera fila y con ruido en la segunda fila de una figura con 10 subfiguras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_caras=5\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "for i in range(n_caras):\n",
    "    plt.subplot(2,n_caras,i+1)\n",
    "    I = np.reshape(data[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off') #quita esta orden para ver número de filas y columnas\n",
    "    \n",
    "    plt.subplot(2,n_caras,i+n_caras+1)\n",
    "    I = np.reshape(data_noisy[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('Ruidosa')\n",
    "    plt.axis('off') #quita esta orden para ver número de filas y columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en la práctica anterior, usaremos las 4.500 primeras caras como ejemplos de entrenamiento y las restantes 500 como test. De esta forma podremos comparar con los resultados obtenidos en la práctica anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tr=4500\n",
    "X_noisy=data_noisy[0:N_tr,:]\n",
    "Test_noisy=data_noisy[N_tr:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de los datos\n",
    "\n",
    "Igual que en el caso de PCA, normalizaremos los datos. Recuerda que en esta ocasión, nuestros datos a normalizar son X_noisy.\n",
    "\n",
    "Usando la clase StandardScaler de sklearn normalizamos los datos de entrenamiento, de forma que cada rasgo (de los 1024) tenga media cero. No vamos a hacer que tenga cada rasgo varianza unidad. Los rasgos normalizados se almacenan en X_norm (fíjate que, como se debe hacer, aplicamos a TODOS los ejemplos la misma normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler(with_mean=True, with_std=False)\n",
    "# Lo ajustamos con los datos de X, y lo aplicamos al mismo\n",
    "# tiempo con fit_transform. \n",
    "# (Lo usaremos luego sobre los datos de test)\n",
    "\n",
    "X_norm = scaler.fit_transform(X_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de las PCAs \n",
    "\n",
    "Una vez que hemos normalizado los datos vamos a calcular todos los vectores y matrices que usa PPCA. \n",
    "\n",
    "Aunque existe una función implementada en el paquete sklearn nosotros no la utilizaremos para comprender en profundidad el funcionamiento de PPCAs. No se considera válido para el desarrollo de la práctica el uso de implementaciones existentes de PPCA.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Debes:\n",
    "    \n",
    "1. Utilizar la descomposición por valores sigulares de X_norm o X_norm traspuesta usando la función np.linalg.svd() y de ahí obtener los autovectores de la matriz de covarianza muestral (mira su definición más abajo) y sus valores singulares. Con estos valores singulares puedes calcular los autovalores de la matriz de covarianza muestral. No te confundas, los valores singulares de X_norm o X_norm transpuesta no son los autovalores de la matriz de covarianza muestral. \n",
    "\n",
    "Alternativamente, \n",
    "    \n",
    "2. Puedes calcular los autovectores y autovalores de la matriz de covarianza muestral  utilizando la función np.linalg.eig(). Tal vez sería bueno que lo hicieras con los métodos y comprobases que obtienes los mismos autovalores y autovectores.</b>\n",
    "</div>\n",
    "\n",
    "Recuerda que dada X_norm su matriz de covarianza muestral es\n",
    "\\begin{equation}\n",
    "S=X_\\mbox{norm}^TX_\\mbox{norm}/N_\\mbox{tr}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduce aqui tu codigo para calcular los autovalores y \n",
    "# los autovectores\n",
    "\n",
    "autovalores=\n",
    "\n",
    "# Introduce en U_all todos los autovectores de la matriz de \n",
    "# covarianza muestral.\n",
    "\n",
    "U_all="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de los estimadores de máxima verosimilitud en PPCA\n",
    "\n",
    "Calculados los autovectores y autovalores de la matriz de covarianza muestral, podemos calcular la matriz $W$ cuyas columnas generan el subespacio principal. Como sabemos, su estimador de máxima verosimilitud (max-likelihood) es:\n",
    "\n",
    "\\begin{equation}\n",
    "W_{ML} = U(L-\\sigma^2I)^{1/2}R \\\\\n",
    "\\end{equation}\n",
    "\n",
    "donde $U$ es una matriz DxM cuyas columnas corresponden a los M autovectores o autocaras asociadas a los M mayores autovalores de la matriz de covarianza muestral. $L$ es una matriz diagonal que contiene los autovalores correspondientes y $R$ es una matriz ortonormal MxM.\n",
    "\n",
    "Por simplicidad, supondremos que $R$ es la matriz identidad, por lo que no es necesario incluirla.\n",
    "\n",
    "Como estimador de $\\sigma^2$ usaremos su estimador de máxima verosimilitud que como sabes de la teoría es\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{ML}^2 = \\frac{1}{D-M}\\sum_{i=M+1}^D\\lambda_i \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Igual que hicimos en la práctica anterior, vamos a utilizar 250 autocaras (M=250), es decir, n_componentes= 250.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Comienza calculando $\\sigma^2_{ML}$ y muestra su valor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de vectores que generarán el subespacio (P)\n",
    "\n",
    "n_componentes=250\n",
    "\n",
    "# Dimensión de las instancias (M)\n",
    "\n",
    "Dimension_datos= data.shape[1]\n",
    "\n",
    "sigma2_ml=\n",
    "\n",
    "mensaje=\"La estimación de varianza del ruido por ML es {}\"\n",
    "print(mensaje.format(sigma2_ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>  ¿Sabrías relacionar el estimador de máxima verosimilitud de la varianza del ruido con la varianza del ruido introducido cuando todavía no habías \n",
    "normalizado los datos?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluye aquí tu respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Obten $U$ de $U_{all}$, $L$ y la estimación de $W$ por máxima verosimilitud, que notaremos $W_{ml}$,  fijando $\\sigma^2$ a $\\sigma^2_{ML}$.</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U =\n",
    "L=\n",
    "\n",
    "W_ml ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de p(z|x)\n",
    "\n",
    "Al utilizar PCA, teniamos un único valor de z para cada muestra x. En PPCA tenemos toda una distribución sobre las z.\n",
    "\n",
    "\\begin{equation}\n",
    "p(z|x)={\\mathcal{N}}(z|M^{-1}W^Tx,\\sigma^2M^{-1}).\n",
    "\\end{equation}\n",
    "\n",
    "donde \n",
    "\n",
    "\\begin{equation}\n",
    "M=W^TW+\\sigma^2I.\n",
    "\\end{equation}\n",
    "\n",
    "Fíjate que aunque la matriz de covarianza es común a todas las muestras, la media varía para cada una de ellas.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Usando los estimadores de máxima verosimilitud que ya hemos calculado, calcula $M$, la media y la matriz de covarianzas de z para todos los datos de X_norm. Almacena todas las medias en mean_Z y la matriz de covarianza común en covar_z. Antes de hacer la implementación, piensa si puedes simplificar el cálculo de alguna de las matrices. Piensa, por ejemplo, en $M$.</b>\n",
    "</div>\n",
    "\n",
    "Observa que la dimensión de mean_Z es 250x4500 ya que contiene la media de esta distribución condicionada por columnas para cada una de las muestras de entrenamiento. Por ello hemos usado mean_Z en lugar de mean_z. La matriz de covarianza covar_z es la misma para todas las muestras, tiene tamaño 250x250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=\n",
    "\n",
    "mean_Z=\n",
    "\n",
    "covar_z=\n",
    "\n",
    "print('mean_Z',mean_Z.shape)\n",
    "print('covar_z',covar_z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dibujar la matriz de covarianza de p(z|x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f500bfdb240>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(covar_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>¿Por qué tiene esa forma?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu respuesta:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre  p(x|z) y su cálculo\n",
    "\n",
    "Como sabes\n",
    "\n",
    "\\begin{equation}\n",
    "p(x|z) = \\mathcal{N}(x|Wz,\\sigma^2I)\\\n",
    "\\end{equation}\n",
    "\n",
    "Estamos usando los estimadores de máxima verosimilitud $W_{ml}$ y $\\sigma^2_{ml}$ que ya hemos calculado. Así pues, esta distribución la podemos calcular muy fácilmente y solo hace falta que nos decidamos por el $z$ que vamos a usar. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Fijando z a las medias que hay en mean_Z calcula la media de cada una de las distribuciones p(x|z) y calcula también la matriz de covarianza de esta distribución. Las llamaremos mean_cond_X y covar_cond_x, respectivamente. Es importante que esté claro para ti cuales son sus dimensiones.</b>\n",
    "</div>\n",
    "\n",
    "Observa que no es eficiente almacenar, por ejemplo, la matriz de covarianza pero nos permite fijar conceptos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cond_X = \n",
    "\n",
    "covar_cond_x =\n",
    "\n",
    "print('mean_cond_X',mean_cond_X.shape)\n",
    "print('covar_cond_x',covar_cond_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con p(x|z) y p(z|x) y comparando con PCA\n",
    "\n",
    "\n",
    "Ya que tenemos todos los ingredientes podemos empezar a utilizar las distribuciones que hemos calculado. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> A partir de X_norm hemos calculado mean_Z y cov_z, es decir, la media y la matriz de covarianza de $p(z|x)$. \n",
    "    \n",
    "1. Usa mean_Z para calcular la media de $p(x|z)$ para todos los ejemplos del conjunto de entrenamiento, almacénalas en X_rec. \n",
    "2. Calcula también las reconstrucciones por PCA. Almacénalas en X_rec_pca \n",
    "\n",
    "Cuando lo tengas, observa que la figura muestra las 5 primeras caras ruidosas de la base de datos original (primera fila), sus reconstrucciones con PPCA (segunda fila), y sus reconstrucción con PCA, (tercera fila). La cuarta fila muestra las imágenes originales.</b>\n",
    "</div>\n",
    "\n",
    "Observa que estamos llevando las imagenes ruidosas observadas a un espacio latente y luego de nuevo al espacio original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para PPCA\n",
    "X_rec=\n",
    "\n",
    "# Para PCA\n",
    "X_rec_pca =\n",
    "\n",
    "n_caras=5\n",
    "n_row=4\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(n_caras):\n",
    "    plt.subplot(n_row,n_caras,i+1)\n",
    "    I = np.reshape(X_noisy[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('Ruidosa')\n",
    "    plt.axis('off')\n",
    "        \n",
    "    plt.subplot(n_row,n_caras,i+1*n_caras+1)\n",
    "    I = np.reshape(X_rec[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('PPCA')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(n_row,n_caras,i+2*n_caras+1)\n",
    "    I = np.reshape(X_rec_pca[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('PCA')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(n_row,n_caras,i+3*n_caras+1)\n",
    "    I = np.reshape(data[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podría pensarse que la reconstrucción de las caras original es mejor con PCA que con PPCA. Piensa sobre quien generalizará mejor a caras no vistas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando p(x|z) para obtener distintas muestras\n",
    "\n",
    "Antes hemos mostrado la media de la distribución $p(x|z)$, pero ahora probaremos a muestrearla.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Has calculado antes mean_cond_X y covar_cond_x. Utiliza la clase multivariate_normal de numpy.random para generar 5 muestras de la primera cara del dataset, debes almacenarlas en X_muestras_1, luego debes deshacer la transformación incial y almacenar las muestras en X_muestras_rec_1. Muestra las 5 imagenes que obtienes.</b>\n",
    "</div>\n",
    "\n",
    "Piensa qué harías si quisieras hacer lo mismo con PCAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_muestras_1=\n",
    "X_muestras_rec_1 = \n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in range(n_caras):\n",
    "    plt.subplot(2,n_caras,i+1)\n",
    "    I = np.reshape(X_muestras_rec_1[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('Muestra p(x|z)')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo generativo\n",
    "\n",
    "Disponer de la distribución a priori $p(z)={\\mathcal{N}}(0,I)$ nos permite generar caras.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Utiliza la clase normal de numpy.random para generar 5 muestras de p(z), las llamamos Z_gen. Par cada una de estas zs muestra la media de p(x|z) y almacénalas todas en X_gen. Luego reconstruye (dehaz la transformación) y muéstralas.</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "Z_gen=np.random.normal(0,1,[n_componentes,5])\n",
    "X_gen=\n",
    "X_gen_rec= \n",
    "\n",
    "n_caras=5\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in range(n_caras):\n",
    "    plt.subplot(2,n_caras,i+1)\n",
    "    I = np.reshape(X_gen_rec[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribución marginal p(x)\n",
    "\n",
    "Integrando en z la distribución conjunta p(x,z) podemos obtener la distribución marginal p(x).\n",
    "\n",
    "\\begin{equation}\n",
    "    p(x)=\\int p(x|z)p(z)\\mbox{d}z={\\cal N}(x|0,C),\n",
    "\\end{equation}\n",
    "\n",
    "donde \n",
    "\n",
    "\\begin{equation}\n",
    "C=WW^T+\\sigma^2I.\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Calcula la matriz C, su determinante y su inversa. Observa que podemos realizar estos cálculos porque el número de rasgos es pequeño. Si tuviésemos menos instancias que rasgos sería mejor usar la identidad de Woodbury y si ambos son muy grandes tendríamos un problema (que es posible solucionar).</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = \n",
    "det_C=\n",
    "C_inv=\n",
    "\n",
    "plt.matshow(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Utiliza la clase multivariate_normal de numpy.random para generar 5 muestras de la distribución marginal p(x), almacénalas en X_muestras. No te confundas, no estás generando necesariamente caras similares a las que has aprendido. Observa que no estás observando una x para generar una z, y a partir de z recuperar x.\n",
    "    \n",
    "Muestra las 5 imagenes que obtienes. No olvides, como siempre, deshacer la normalización. Las caras sin la normalización debes almacenarlas en X_muestras_rec.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_muestras=\n",
    "X_muestras_rec=\n",
    "\n",
    "n_caras=5\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(n_caras):\n",
    "    plt.subplot(2,n_caras,i+1)\n",
    "    I = np.reshape(X_muestras_rec[i,:],[32,32]).T\n",
    "    plt.imshow(I,cmap='gray')\n",
    "    plt.title('Muestra de p(x)')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ln P(x)\n",
    "\n",
    "Estimados $W_{ml}$  y $\\sigma^2_{ml}$ podemos calcular el logaritmo de la probabilidad de una observación x:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln p(x|W_{\\mbox{ML}},sigma_{\\mbox{ML}}^2)=-\\frac{M}{2}\\ln(2\\pi)-\\frac{1}{2}\\ln|C|-\\frac{1}{2}\\color{red}{x^TC^{-1}x} \n",
    "\\end{equation} \n",
    "\n",
    "Puede usarse para identificar anomalías con respecto a las imágenes que hemos utilizado para aprender nuestro modelo. Como todos los términos de la derecha de la ecuación anterior son constantes salvo en que esta en rojo, vamos a calcularlo.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Calcula $x^TC^{-1}x$ para cada muestra x en X_norm y almacenalo en X_error que obviamente será un vector con tantas componentes como datos en el conjunto de entrenamiento, es decir (4500,).</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_error="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando los errores que has calculado vamos a mostrar la cara que tiene mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min_error=np.argmin(X_error)\n",
    "print(\"Índice de la cara más probable: {}\".format(1+index_min_error))\n",
    "I = np.reshape(X_rec[index_min_error,:],[32,32]).T\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.title('Más probable')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando los errores que has calculado vamos a mostrar la cara que tiene menor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_max_error=np.argmax(X_error)\n",
    "print(\"Índice de la cara menos probable: {}\".format(1+index_max_error))\n",
    "I = np.reshape(X_rec[index_max_error,:],[32,32]).T\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.title('Menos probable')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Para terminar este guión vamos a ver como de probables son nuestras datos de test.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Encuentra y muestra la cara de test que es más probable y la menos probable, con el modelo que hemos construido.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para terminar\n",
    "\n",
    "Nos hemos dejado muchas cosas interesantes por hacer. Te sugiero que pienses en las siguientes y escribas el código que te permita analizar lo que está pasando. Si quieres inclúyelo aquí [OPCIONAL]. Te dejo dos preguntas:\n",
    "\n",
    "1. ¿Qué diferencias habrá entre el espacio latente de las PCAs y de las PPCAs. Sí, el de las PCAs es determinista. ¿Cuáles serán las implicaciones?\n",
    "\n",
    "2. Piensa en la base de datos MNIST. ¿Qué te gustaría que pasase en el espacio latente de las PPCA?, ¿Sería bueno que cada dígito estuviera separado, en su representación latente, de los otros dígitos?\n",
    "\n",
    "3. Sí, he dicho dos pero una más para terminar. ¿Se te ocurriría alguna forma de pasar de una cara a otra (o de un número a otro si usamos MNIST) en el espacio latente? Piensa en la segunda pregunta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
